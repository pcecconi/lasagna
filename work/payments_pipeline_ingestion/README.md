# ğŸ¦ Payments Pipeline Ingestion

ELT-based data pipeline for payments aggregator service using Apache Iceberg.

## ğŸ¯ **Architecture Overview**

### **ELT Approach:**
- **Extract**: Raw CSV files from data generator
- **Load**: Ingest into Iceberg tables with minimal transformation
- **Transform**: Data modeling and business logic in Silver layer

### **Pipeline Layers:**
- **ğŸ¥‰ Bronze Layer**: Raw data ingestion (this repository)
- **ğŸ¥ˆ Silver Layer**: Data transformation and modeling
- **ğŸ¥‡ Gold Layer**: Analytics and reporting

## ğŸ“ **Repository Structure**

```
payments_pipeline_ingestion/
â”œâ”€â”€ src/payments_pipeline/
â”‚   â”œâ”€â”€ bronze/                    # Bronze layer modules
â”‚   â”‚   â”œâ”€â”€ ingestion.py          # Raw data ingestion
â”‚   â”‚   â””â”€â”€ validation.py         # Data quality checks
â”‚   â”œâ”€â”€ silver/                    # Silver layer modules (future)
â”‚   â”œâ”€â”€ gold/                      # Gold layer modules (future)
â”‚   â””â”€â”€ utils/                     # Shared utilities
â”‚       â”œâ”€â”€ spark.py              # Spark session management
â”‚       â”œâ”€â”€ config.py             # Configuration management
â”‚       â””â”€â”€ logging.py            # Logging utilities
â”œâ”€â”€ tests/                         # Unit and integration tests
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for exploration
â”œâ”€â”€ scripts/                       # Standalone scripts
â”œâ”€â”€ configs/                       # Configuration files
â””â”€â”€ docs/                          # Documentation
```

## ğŸš€ **Quick Start**

### **Prerequisites:**
- Lasagna stack running (Docker containers)
- Raw data generated by data generator
- JupyterLab access

### **1. Access JupyterLab**
```bash
# Start lasagna stack
cd /path/to/lasagna
docker compose up -d

# Access JupyterLab at http://localhost:8888
```

### **2. Run Bronze Ingestion**
```python
# In JupyterLab notebook
import sys
sys.path.append('/usr/local/spark_dev/work/payments_pipeline_ingestion/src')

from payments_pipeline.bronze.ingestion import BronzeIngestionJob

# Initialize and run
bronze_job = BronzeIngestionJob()
bronze_job.ingest_batch('/usr/local/spark_dev/work/payments_pipeline/raw_data')
```

### **3. Or Use Standalone Script**
```bash
# In container
python /usr/local/spark_dev/work/payments_pipeline_ingestion/scripts/run_bronze_ingestion.py
```

## ğŸ¥‰ **Bronze Layer Features**

### **Raw Data Ingestion:**
- Preserves original CSV structure
- Minimal transformation (ELT approach)
- Schema inference from CSV files

### **Metadata Tracking:**
- `ingestion_timestamp`: When data was ingested
- `source_file`: Original file name
- `bronze_layer_version`: Version tracking
- `data_source`: Source system identifier

### **Data Quality:**
- Basic validation checks
- Null value detection
- Schema validation
- Row count verification

### **Storage:**
- Apache Iceberg tables for ACID properties
- Partitioning support for performance
- Time travel capabilities
- Schema evolution support

## ğŸ“Š **Data Flow**

```
Raw CSV Files                    Bronze Layer                    Iceberg Tables
==============                    =============                    ==============
merchants_*.csv    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   Raw Ingestion    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   merchants_raw
transactions_*.csv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   + Metadata       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   transactions_raw
```

## ğŸ”§ **Configuration**

### **Environment Variables:**
```bash
SPARK_MASTER=spark://spark-master:7077
ICEBERG_CATALOG=iceberg
ICEBERG_URI=thrift://hive-metastore:9083
WAREHOUSE_DIR=s3a://warehouse/
S3_ENDPOINT=http://minio:9000
```

### **Configuration File:**
```yaml
# configs/development.yml
spark_master: "spark://spark-master:7077"
iceberg_catalog: "iceberg"
bronze_namespace: "payments_bronze"
warehouse_dir: "s3a://warehouse/"
```

## ğŸ“‹ **Available Tables**

### **Bronze Layer Tables:**
- `iceberg.payments_bronze.merchants_raw`
- `iceberg.payments_bronze.transactions_raw`

### **Table Schemas:**
```sql
-- Merchants Raw Table
CREATE TABLE iceberg.payments_bronze.merchants_raw (
    merchant_id STRING,
    merchant_name STRING,
    industry STRING,
    address STRING,
    city STRING,
    state STRING,
    zip_code STRING,
    phone STRING,
    email STRING,
    mdr_rate DOUBLE,
    size_category STRING,
    creation_date STRING,
    status STRING,
    last_transaction_date STRING,
    ingestion_timestamp TIMESTAMP,
    source_file STRING,
    bronze_layer_version STRING,
    data_source STRING
) USING iceberg;

-- Transactions Raw Table
CREATE TABLE iceberg.payments_bronze.transactions_raw (
    payment_id STRING,
    payment_timestamp STRING,
    payment_lat DOUBLE,
    payment_lng DOUBLE,
    payment_amount DOUBLE,
    payment_type STRING,
    terminal_id STRING,
    card_type STRING,
    card_issuer STRING,
    card_brand STRING,
    payment_status STRING,
    merchant_id STRING,
    transactional_cost_rate DOUBLE,
    transactional_cost_amount DOUBLE,
    mdr_amount DOUBLE,
    net_profit DOUBLE,
    ingestion_timestamp TIMESTAMP,
    source_file STRING,
    bronze_layer_version STRING,
    data_source STRING
) USING iceberg;
```

## ğŸ§ª **Testing**

### **Run Tests:**
```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# All tests with coverage
pytest tests/ --cov=src/ --cov-report=html
```

### **Validation:**
```python
# Validate ingested data
bronze_job.validate_ingestion("iceberg.payments_bronze.merchants_raw")
bronze_job.validate_ingestion("iceberg.payments_bronze.transactions_raw")
```

## ğŸ”„ **Incremental Processing**

### **Add New Data:**
```python
# Ingest incremental transaction file
bronze_job.ingest_incremental_transactions("path/to/transactions_20240115.csv")
```

### **Batch Processing:**
```python
# Process all files in directory
bronze_job.ingest_batch("/path/to/data/directory")
```

## ğŸ“ˆ **Performance Optimization**

### **Iceberg Features:**
- **Hidden Partitioning**: Automatic optimization
- **ACID Transactions**: Data consistency
- **Time Travel**: Historical data access
- **Schema Evolution**: Add/modify columns
- **Compaction**: Automatic file optimization

### **Spark Configuration:**
- Adaptive Query Execution
- Dynamic Partition Pruning
- Broadcast Joins
- Memory optimization

## ğŸš€ **Next Steps**

1. **Silver Layer**: Transform bronze data into star schema
2. **SCD Type 2**: Implement slowly changing dimensions for merchants
3. **Data Quality**: Advanced validation rules
4. **Monitoring**: Pipeline monitoring and alerting
5. **Gold Layer**: Analytics and reporting tables

## ğŸ“š **Documentation**

- [Bronze Layer Design](docs/bronze_layer_design.md)
- [Iceberg Configuration](docs/iceberg_config.md)
- [Data Quality Rules](docs/data_quality.md)
- [Performance Tuning](docs/performance.md)

## ğŸ¤ **Contributing**

1. Follow ELT principles (minimal transformation in bronze)
2. Add tests for new functionality
3. Update documentation
4. Follow coding standards

## ğŸ“„ **License**

This project is part of the Lasagna big data learning environment.
