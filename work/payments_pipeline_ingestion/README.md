# 🏦 Payments Pipeline Ingestion

ELT-based data pipeline for payments aggregator service using Apache Iceberg.

## 🎯 **Architecture Overview**

### **ELT Approach:**
- **Extract**: Raw CSV files from data generator
- **Load**: Ingest into Iceberg tables with minimal transformation
- **Transform**: Data modeling and business logic in Silver layer

### **Pipeline Layers:**
- **🥉 Bronze Layer**: Raw data ingestion (this repository)
- **🥈 Silver Layer**: Data transformation and modeling
- **🥇 Gold Layer**: Analytics and reporting

## 📁 **Repository Structure**

```
payments_pipeline_ingestion/
├── src/payments_pipeline/
│   ├── bronze/                    # Bronze layer modules
│   │   ├── ingestion.py          # Raw data ingestion
│   │   └── validation.py         # Data quality checks
│   ├── silver/                    # Silver layer modules (future)
│   ├── gold/                      # Gold layer modules (future)
│   └── utils/                     # Shared utilities
│       ├── spark.py              # Spark session management
│       ├── config.py             # Configuration management
│       └── logging.py            # Logging utilities
├── tests/                         # Unit and integration tests
├── notebooks/                     # Jupyter notebooks for exploration
├── scripts/                       # Standalone scripts
├── configs/                       # Configuration files
└── docs/                          # Documentation
```

## 🚀 **Quick Start**

### **Prerequisites:**
- Lasagna stack running (Docker containers)
- Raw data generated by data generator
- JupyterLab access

### **1. Access JupyterLab**
```bash
# Start lasagna stack
cd /path/to/lasagna
docker compose up -d

# Access JupyterLab at http://localhost:8888
```

### **2. Run Bronze Ingestion**
```python
# In JupyterLab notebook
import sys
sys.path.append('/usr/local/spark_dev/work/payments_pipeline_ingestion/src')

from payments_pipeline.bronze.ingestion import BronzeIngestionJob

# Initialize and run
bronze_job = BronzeIngestionJob()
bronze_job.ingest_batch('/usr/local/spark_dev/work/payments_pipeline/raw_data')
```

### **3. Or Use Standalone Script**
```bash
# In container
python /usr/local/spark_dev/work/payments_pipeline_ingestion/scripts/run_bronze_ingestion.py
```

## 🥉 **Bronze Layer Features**

### **Raw Data Ingestion:**
- Preserves original CSV structure
- Minimal transformation (ELT approach)
- Schema inference from CSV files

### **Metadata Tracking:**
- `ingestion_timestamp`: When data was ingested
- `source_file`: Original file name
- `bronze_layer_version`: Version tracking
- `data_source`: Source system identifier

### **Data Quality:**
- Basic validation checks
- Null value detection
- Schema validation
- Row count verification

### **Storage:**
- Apache Iceberg tables for ACID properties
- Partitioning support for performance
- Time travel capabilities
- Schema evolution support

## 📊 **Data Flow**

```
Raw CSV Files                    Bronze Layer                    Iceberg Tables
==============                    =============                    ==============
merchants_*.csv    ──────────►   Raw Ingestion    ──────────►   merchants_raw
transactions_*.csv ──────────►   + Metadata       ──────────►   transactions_raw
```

## 🔧 **Configuration**

### **Environment Variables:**
```bash
SPARK_MASTER=spark://spark-master:7077
ICEBERG_CATALOG=iceberg
ICEBERG_URI=thrift://hive-metastore:9083
WAREHOUSE_DIR=s3a://warehouse/
S3_ENDPOINT=http://minio:9000
```

### **Configuration File:**
```yaml
# configs/development.yml
spark_master: "spark://spark-master:7077"
iceberg_catalog: "iceberg"
bronze_namespace: "payments_bronze"
warehouse_dir: "s3a://warehouse/"
```

## 📋 **Available Tables**

### **Bronze Layer Tables:**
- `iceberg.payments_bronze.merchants_raw`
- `iceberg.payments_bronze.transactions_raw`

### **Table Schemas:**
```sql
-- Merchants Raw Table
CREATE TABLE iceberg.payments_bronze.merchants_raw (
    merchant_id STRING,
    merchant_name STRING,
    industry STRING,
    address STRING,
    city STRING,
    state STRING,
    zip_code STRING,
    phone STRING,
    email STRING,
    mdr_rate DOUBLE,
    size_category STRING,
    creation_date STRING,
    status STRING,
    last_transaction_date STRING,
    ingestion_timestamp TIMESTAMP,
    source_file STRING,
    bronze_layer_version STRING,
    data_source STRING
) USING iceberg;

-- Transactions Raw Table
CREATE TABLE iceberg.payments_bronze.transactions_raw (
    payment_id STRING,
    payment_timestamp STRING,
    payment_lat DOUBLE,
    payment_lng DOUBLE,
    payment_amount DOUBLE,
    payment_type STRING,
    terminal_id STRING,
    card_type STRING,
    card_issuer STRING,
    card_brand STRING,
    payment_status STRING,
    merchant_id STRING,
    transactional_cost_rate DOUBLE,
    transactional_cost_amount DOUBLE,
    mdr_amount DOUBLE,
    net_profit DOUBLE,
    ingestion_timestamp TIMESTAMP,
    source_file STRING,
    bronze_layer_version STRING,
    data_source STRING
) USING iceberg;
```

## 🧪 **Testing**

### **Run Tests:**
```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# All tests with coverage
pytest tests/ --cov=src/ --cov-report=html
```

### **Validation:**
```python
# Validate ingested data
bronze_job.validate_ingestion("iceberg.payments_bronze.merchants_raw")
bronze_job.validate_ingestion("iceberg.payments_bronze.transactions_raw")
```

## 🔄 **Incremental Processing**

### **Add New Data:**
```python
# Ingest incremental transaction file
bronze_job.ingest_incremental_transactions("path/to/transactions_20240115.csv")
```

### **Batch Processing:**
```python
# Process all files in directory
bronze_job.ingest_batch("/path/to/data/directory")
```

## 📈 **Performance Optimization**

### **Iceberg Features:**
- **Hidden Partitioning**: Automatic optimization
- **ACID Transactions**: Data consistency
- **Time Travel**: Historical data access
- **Schema Evolution**: Add/modify columns
- **Compaction**: Automatic file optimization

### **Spark Configuration:**
- Adaptive Query Execution
- Dynamic Partition Pruning
- Broadcast Joins
- Memory optimization

## 🚀 **Next Steps**

1. **Silver Layer**: Transform bronze data into star schema
2. **SCD Type 2**: Implement slowly changing dimensions for merchants
3. **Data Quality**: Advanced validation rules
4. **Monitoring**: Pipeline monitoring and alerting
5. **Gold Layer**: Analytics and reporting tables

## 📚 **Documentation**

- [Bronze Layer Design](docs/bronze_layer_design.md)
- [Iceberg Configuration](docs/iceberg_config.md)
- [Data Quality Rules](docs/data_quality.md)
- [Performance Tuning](docs/performance.md)

## 🤝 **Contributing**

1. Follow ELT principles (minimal transformation in bronze)
2. Add tests for new functionality
3. Update documentation
4. Follow coding standards

## 📄 **License**

This project is part of the Lasagna big data learning environment.
