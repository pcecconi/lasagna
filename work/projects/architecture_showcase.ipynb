{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LASAGNA Big Data Architecture Showcase\n",
        "\n",
        "This notebook demonstrates and tests all components of the LASAGNA big data architecture:\n",
        "\n",
        "## Architecture Components Tested:\n",
        "- **MinIO**: S3-compatible object storage\n",
        "- **PostgreSQL**: Metadata database\n",
        "- **Hive Metastore**: Centralized catalog service\n",
        "- **Spark Cluster**: Distributed processing engine\n",
        "- **Trino**: Fast analytical query engine\n",
        "- **JupyterLab**: Interactive development environment\n",
        "\n",
        "## Table Formats Demonstrated:\n",
        "- **Hive Tables**: Traditional format\n",
        "- **Delta Lake**: ACID transactions and time travel\n",
        "- **Apache Iceberg**: Schema evolution and advanced partitioning\n",
        "\n",
        "## What We'll Test:\n",
        "1. Connection to all services\n",
        "2. Sample data creation\n",
        "3. Table operations across different formats\n",
        "4. Cross-engine querying with Trino\n",
        "5. Performance comparisons\n",
        "6. Advanced features (ACID, time travel, schema evolution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Connection Tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import trino\n",
        "import json\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test connections to all services\n",
        "def test_service_connections():\n",
        "    services = {\n",
        "        \"MinIO Console\": \"http://localhost:9090\",\n",
        "        \"Spark Master\": \"http://localhost:5050\",\n",
        "        \"Spark Worker A\": \"http://localhost:5051\",\n",
        "        \"Spark Worker B\": \"http://localhost:5052\",\n",
        "        \"Trino\": \"http://localhost:8080\"\n",
        "    }\n",
        "    \n",
        "    print(\"üîç Testing service connections...\")\n",
        "    for service, url in services.items():\n",
        "        try:\n",
        "            response = requests.get(url, timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"‚úÖ {service}: Connected\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  {service}: Responding but status {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {service}: Connection failed - {str(e)}\")\n",
        "\n",
        "test_service_connections()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sample Data Generation\n",
        "\n",
        "Let's create realistic sample datasets to test our architecture:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample employee data\n",
        "def generate_employee_data(num_records=10000):\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    departments = ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance', 'Operations']\n",
        "    positions = ['Manager', 'Senior', 'Mid-level', 'Junior', 'Intern']\n",
        "    locations = ['New York', 'San Francisco', 'London', 'Tokyo', 'Berlin']\n",
        "    \n",
        "    data = []\n",
        "    base_date = datetime(2020, 1, 1)\n",
        "    \n",
        "    for i in range(num_records):\n",
        "        hire_date = base_date + timedelta(days=np.random.randint(0, 1460))  # 4 years range\n",
        "        data.append({\n",
        "            'employee_id': f'EMP_{i+1:06d}',\n",
        "            'first_name': f'Employee{i+1}',\n",
        "            'last_name': f'LastName{i+1}',\n",
        "            'email': f'employee{i+1}@company.com',\n",
        "            'department': np.random.choice(departments),\n",
        "            'position': np.random.choice(positions),\n",
        "            'location': np.random.choice(locations),\n",
        "            'salary': np.random.randint(40000, 200000),\n",
        "            'hire_date': hire_date.strftime('%Y-%m-%d'),\n",
        "            'is_active': np.random.choice([True, False], p=[0.85, 0.15])\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate sample sales data\n",
        "def generate_sales_data(num_records=50000):\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    products = ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse', 'Headphones']\n",
        "    regions = ['North America', 'Europe', 'Asia', 'South America', 'Africa']\n",
        "    \n",
        "    data = []\n",
        "    base_date = datetime(2023, 1, 1)\n",
        "    \n",
        "    for i in range(num_records):\n",
        "        sale_date = base_date + timedelta(days=np.random.randint(0, 365))\n",
        "        quantity = np.random.randint(1, 10)\n",
        "        unit_price = np.random.uniform(50, 2000)\n",
        "        \n",
        "        data.append({\n",
        "            'sale_id': f'SALE_{i+1:08d}',\n",
        "            'product': np.random.choice(products),\n",
        "            'quantity': quantity,\n",
        "            'unit_price': round(unit_price, 2),\n",
        "            'total_amount': round(quantity * unit_price, 2),\n",
        "            'region': np.random.choice(regions),\n",
        "            'sale_date': sale_date.strftime('%Y-%m-%d'),\n",
        "            'customer_id': f'CUST_{np.random.randint(1, 5000):06d}'\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "print(\"üìä Sample data generation functions created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the sample datasets\n",
        "print(\"üîÑ Generating sample datasets...\")\n",
        "\n",
        "employees_df = generate_employee_data(10000)\n",
        "sales_df = generate_sales_data(50000)\n",
        "\n",
        "print(f\"‚úÖ Generated {len(employees_df):,} employee records\")\n",
        "print(f\"‚úÖ Generated {len(sales_df):,} sales records\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nüìã Sample Employee Data:\")\n",
        "print(employees_df.head())\n",
        "\n",
        "print(\"\\nüìã Sample Sales Data:\")\n",
        "print(sales_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Spark Session Setup and Hive Tables\n",
        "\n",
        "Let's initialize Spark and create Hive tables to test the traditional data warehouse functionality:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session with Hive support\n",
        "spark = SparkSession.builder.appName(\"LASAGNA-Architecture-Test\").getOrCreate()\n",
        "\n",
        "print(\"üöÄ Spark Session initialized\")\n",
        "print(f\"üìä Spark Version: {spark.version}\")\n",
        "print(f\"üîó Spark Master: {spark.conf.get('spark.master')}\")\n",
        "print(f\"üìÅ Warehouse Directory: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
        "\n",
        "# Test Hive Metastore connection\n",
        "try:\n",
        "    spark.sql(\"SHOW DATABASES\").show()\n",
        "    print(\"‚úÖ Hive Metastore connection successful\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Hive Metastore connection failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create database and Hive tables\n",
        "print(\"üèóÔ∏è Creating database and Hive tables...\")\n",
        "\n",
        "# Create database\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS lasagna_demo\")\n",
        "spark.sql(\"USE lasagna_demo\")\n",
        "\n",
        "# Convert pandas DataFrames to Spark DataFrames\n",
        "employees_spark = spark.createDataFrame(employees_df)\n",
        "sales_spark = spark.createDataFrame(sales_df)\n",
        "\n",
        "# Create Hive tables\n",
        "print(\"üìù Creating employees_hive table...\")\n",
        "employees_spark.write.mode(\"overwrite\").saveAsTable(\"employees_hive\")\n",
        "\n",
        "print(\"üìù Creating sales_hive table...\")\n",
        "sales_spark.write.mode(\"overwrite\").saveAsTable(\"sales_hive\")\n",
        "\n",
        "print(\"‚úÖ Hive tables created successfully\")\n",
        "\n",
        "# Verify tables\n",
        "print(\"\\nüìã Available tables:\")\n",
        "spark.sql(\"SHOW TABLES\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Hive table queries\n",
        "print(\"üîç Testing Hive table queries...\")\n",
        "\n",
        "# Basic queries\n",
        "print(\"\\n1Ô∏è‚É£ Employee count by department:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT department, COUNT(*) as employee_count \n",
        "    FROM employees_hive \n",
        "    GROUP BY department \n",
        "    ORDER BY employee_count DESC\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Average salary by position:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT position, ROUND(AVG(salary), 2) as avg_salary \n",
        "    FROM employees_hive \n",
        "    GROUP BY position \n",
        "    ORDER BY avg_salary DESC\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Sales summary by region:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT region, \n",
        "           COUNT(*) as total_sales,\n",
        "           ROUND(SUM(total_amount), 2) as total_revenue,\n",
        "           ROUND(AVG(total_amount), 2) as avg_sale_amount\n",
        "    FROM sales_hive \n",
        "    GROUP BY region \n",
        "    ORDER BY total_revenue DESC\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ Top selling products:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT product, \n",
        "           COUNT(*) as sales_count,\n",
        "           ROUND(SUM(total_amount), 2) as total_revenue\n",
        "    FROM sales_hive \n",
        "    GROUP BY product \n",
        "    ORDER BY total_revenue DESC\n",
        "    LIMIT 5\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Delta Lake - ACID Transactions and Time Travel\n",
        "\n",
        "Now let's test Delta Lake capabilities including ACID transactions and time travel:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Delta Lake tables\n",
        "print(\"üîÑ Creating Delta Lake tables...\")\n",
        "\n",
        "# Create Delta tables\n",
        "employees_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employees_delta\")\n",
        "sales_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_delta\")\n",
        "\n",
        "print(\"‚úÖ Delta Lake tables created successfully\")\n",
        "\n",
        "# Show Delta table history\n",
        "print(\"\\nüìú Delta table history (employees_delta):\")\n",
        "spark.sql(\"DESCRIBE HISTORY employees_delta\").show()\n",
        "\n",
        "print(\"\\nüìú Delta table history (sales_delta):\")\n",
        "spark.sql(\"DESCRIBE HISTORY sales_delta\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate ACID transactions and updates\n",
        "print(\"üîí Testing ACID transactions...\")\n",
        "\n",
        "# Record current version\n",
        "current_version = spark.sql(\"DESCRIBE HISTORY employees_delta\").collect()[0]['version']\n",
        "print(f\"üìä Current version: {current_version}\")\n",
        "\n",
        "# Perform updates (ACID transaction)\n",
        "print(\"\\nüîÑ Performing updates...\")\n",
        "\n",
        "# Update salaries for Engineering department\n",
        "spark.sql(\"\"\"\n",
        "    UPDATE employees_delta \n",
        "    SET salary = salary * 1.1 \n",
        "    WHERE department = 'Engineering'\n",
        "\"\"\")\n",
        "\n",
        "# Insert new employee\n",
        "spark.sql(\"\"\"\n",
        "    INSERT INTO employees_delta VALUES \n",
        "    ('EMP_999999', 'New', 'Employee', 'new.employee@company.com', \n",
        "     'Engineering', 'Senior', 'San Francisco', 120000, '2024-01-15', true)\n",
        "\"\"\")\n",
        "\n",
        "# Delete inactive employees from HR\n",
        "spark.sql(\"\"\"\n",
        "    DELETE FROM employees_delta \n",
        "    WHERE department = 'HR' AND is_active = false\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ ACID transactions completed\")\n",
        "\n",
        "# Show updated history\n",
        "print(\"\\nüìú Updated Delta table history:\")\n",
        "spark.sql(\"DESCRIBE HISTORY employees_delta\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate Time Travel\n",
        "print(\"‚è∞ Testing Delta Lake Time Travel...\")\n",
        "\n",
        "# Show current data\n",
        "print(\"\\nüìä Current Engineering employees count:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT COUNT(*) as current_count \n",
        "    FROM employees_delta \n",
        "    WHERE department = 'Engineering'\n",
        "\"\"\").show()\n",
        "\n",
        "# Travel back to version 0 (original data)\n",
        "print(\"\\nüï∞Ô∏è Time traveling to version 0...\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT COUNT(*) as original_count \n",
        "    FROM employees_delta VERSION AS OF 0 \n",
        "    WHERE department = 'Engineering'\n",
        "\"\"\").show()\n",
        "\n",
        "# Compare average salaries\n",
        "print(\"\\nüí∞ Salary comparison - Current vs Original:\")\n",
        "print(\"Current average salary for Engineering:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT ROUND(AVG(salary), 2) as current_avg_salary \n",
        "    FROM employees_delta \n",
        "    WHERE department = 'Engineering'\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"Original average salary for Engineering:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT ROUND(AVG(salary), 2) as original_avg_salary \n",
        "    FROM employees_delta VERSION AS OF 0 \n",
        "    WHERE department = 'Engineering'\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"‚úÖ Time travel demonstration completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Apache Iceberg - Schema Evolution and Advanced Partitioning\n",
        "\n",
        "Let's test Iceberg's advanced features including schema evolution and partitioning:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Iceberg tables with partitioning\n",
        "print(\"üßä Creating Iceberg tables with partitioning...\")\n",
        "\n",
        "# Create Iceberg catalog\n",
        "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.lasagna_demo\")\n",
        "\n",
        "# Create partitioned Iceberg table\n",
        "print(\"üìù Creating partitioned employees_iceberg table...\")\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE iceberg.lasagna_demo.employees_iceberg (\n",
        "        employee_id STRING,\n",
        "        first_name STRING,\n",
        "        last_name STRING,\n",
        "        email STRING,\n",
        "        department STRING,\n",
        "        position STRING,\n",
        "        location STRING,\n",
        "        salary INT,\n",
        "        hire_date DATE,\n",
        "        is_active BOOLEAN\n",
        "    ) USING iceberg\n",
        "    PARTITIONED BY (department, location)\n",
        "\"\"\")\n",
        "\n",
        "# Insert data into Iceberg table\n",
        "employees_spark.writeTo(\"iceberg.lasagna_demo.employees_iceberg\").append()\n",
        "\n",
        "print(\"‚úÖ Iceberg table created and populated\")\n",
        "\n",
        "# Show table details\n",
        "print(\"\\nüìã Iceberg table details:\")\n",
        "spark.sql(\"DESCRIBE TABLE EXTENDED iceberg.lasagna_demo.employees_iceberg\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate Schema Evolution\n",
        "print(\"üîÑ Testing Iceberg Schema Evolution...\")\n",
        "\n",
        "# Add new columns\n",
        "print(\"\\n‚ûï Adding new columns to Iceberg table...\")\n",
        "spark.sql(\"\"\"\n",
        "    ALTER TABLE iceberg.lasagna_demo.employees_iceberg \n",
        "    ADD COLUMN bonus DECIMAL(10,2),\n",
        "    ADD COLUMN performance_rating STRING\n",
        "\"\"\")\n",
        "\n",
        "# Update existing records with new columns\n",
        "spark.sql(\"\"\"\n",
        "    UPDATE iceberg.lasagna_demo.employees_iceberg \n",
        "    SET bonus = CASE \n",
        "        WHEN position = 'Manager' THEN salary * 0.2\n",
        "        WHEN position = 'Senior' THEN salary * 0.15\n",
        "        ELSE salary * 0.1\n",
        "    END,\n",
        "    performance_rating = CASE \n",
        "        WHEN salary > 150000 THEN 'Excellent'\n",
        "        WHEN salary > 100000 THEN 'Good'\n",
        "        ELSE 'Average'\n",
        "    END\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Schema evolution completed\")\n",
        "\n",
        "# Show updated schema\n",
        "print(\"\\nüìã Updated table schema:\")\n",
        "spark.sql(\"DESCRIBE iceberg.lasagna_demo.employees_iceberg\").show()\n",
        "\n",
        "# Test queries on evolved schema\n",
        "print(\"\\nüîç Testing queries on evolved schema:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT department, \n",
        "           COUNT(*) as employee_count,\n",
        "           ROUND(AVG(bonus), 2) as avg_bonus,\n",
        "           COUNT(CASE WHEN performance_rating = 'Excellent' THEN 1 END) as excellent_performers\n",
        "    FROM iceberg.lasagna_demo.employees_iceberg \n",
        "    GROUP BY department \n",
        "    ORDER BY avg_bonus DESC\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Trino - Cross-Engine Querying\n",
        "\n",
        "Let's test Trino's ability to query across all our table formats:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to Trino\n",
        "print(\"üîó Connecting to Trino...\")\n",
        "\n",
        "try:\n",
        "    conn = trino.dbapi.connect(\n",
        "        host='localhost',\n",
        "        port=8080,\n",
        "        user='admin',\n",
        "        catalog='hive',\n",
        "        schema='lasagna_demo'\n",
        "    )\n",
        "    \n",
        "    cursor = conn.cursor()\n",
        "    print(\"‚úÖ Trino connection successful\")\n",
        "    \n",
        "    # Test basic query\n",
        "    cursor.execute(\"SHOW TABLES\")\n",
        "    tables = cursor.fetchall()\n",
        "    print(f\"\\nüìã Available tables in Trino: {[table[0] for table in tables]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Trino connection failed: {e}\")\n",
        "    print(\"üí° Make sure Trino is running on localhost:8080\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Trino queries across different catalogs\n",
        "if 'conn' in locals():\n",
        "    print(\"üîç Testing Trino queries across catalogs...\")\n",
        "    \n",
        "    # Query Hive tables\n",
        "    print(\"\\n1Ô∏è‚É£ Querying Hive tables:\")\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT department, COUNT(*) as employee_count \n",
        "        FROM hive.lasagna_demo.employees_hive \n",
        "        GROUP BY department \n",
        "        ORDER BY employee_count DESC\n",
        "        LIMIT 5\n",
        "    \"\"\")\n",
        "    hive_results = cursor.fetchall()\n",
        "    print(\"Hive table results:\")\n",
        "    for row in hive_results:\n",
        "        print(f\"  {row[0]}: {row[1]} employees\")\n",
        "    \n",
        "    # Query Delta tables\n",
        "    print(\"\\n2Ô∏è‚É£ Querying Delta tables:\")\n",
        "    try:\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT department, COUNT(*) as employee_count \n",
        "            FROM delta_lake.lasagna_demo.employees_delta \n",
        "            GROUP BY department \n",
        "            ORDER BY employee_count DESC\n",
        "            LIMIT 5\n",
        "        \"\"\")\n",
        "        delta_results = cursor.fetchall()\n",
        "        print(\"Delta table results:\")\n",
        "        for row in delta_results:\n",
        "            print(f\"  {row[0]}: {row[1]} employees\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Delta query failed: {e}\")\n",
        "    \n",
        "    # Query Iceberg tables\n",
        "    print(\"\\n3Ô∏è‚É£ Querying Iceberg tables:\")\n",
        "    try:\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT department, COUNT(*) as employee_count \n",
        "            FROM iceberg.lasagna_demo.employees_iceberg \n",
        "            GROUP BY department \n",
        "            ORDER BY employee_count DESC\n",
        "            LIMIT 5\n",
        "        \"\"\")\n",
        "        iceberg_results = cursor.fetchall()\n",
        "        print(\"Iceberg table results:\")\n",
        "        for row in iceberg_results:\n",
        "            print(f\"  {row[0]}: {row[1]} employees\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Iceberg query failed: {e}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Trino cross-catalog querying completed\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping Trino tests - connection not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Comparison\n",
        "\n",
        "Let's compare query performance across different table formats:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance comparison function\n",
        "def measure_query_performance(query, table_name, description):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        result = spark.sql(query)\n",
        "        result.collect()  # Force execution\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        print(f\"‚úÖ {description} ({table_name}): {execution_time:.2f} seconds\")\n",
        "        return execution_time\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {description} ({table_name}): Failed - {e}\")\n",
        "        return None\n",
        "\n",
        "# Test query performance\n",
        "print(\"‚ö° Performance comparison across table formats...\")\n",
        "\n",
        "# Complex analytical query\n",
        "complex_query = \"\"\"\n",
        "    SELECT department, \n",
        "           location,\n",
        "           COUNT(*) as employee_count,\n",
        "           ROUND(AVG(salary), 2) as avg_salary,\n",
        "           ROUND(MAX(salary), 2) as max_salary,\n",
        "           ROUND(MIN(salary), 2) as min_salary\n",
        "    FROM {table} \n",
        "    WHERE is_active = true\n",
        "    GROUP BY department, location\n",
        "    HAVING COUNT(*) > 50\n",
        "    ORDER BY avg_salary DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîç Running complex analytical queries...\")\n",
        "\n",
        "# Test Hive performance\n",
        "hive_time = measure_query_performance(\n",
        "    complex_query.format(table=\"employees_hive\"),\n",
        "    \"Hive\",\n",
        "    \"Complex Analytics Query\"\n",
        ")\n",
        "\n",
        "# Test Delta performance\n",
        "delta_time = measure_query_performance(\n",
        "    complex_query.format(table=\"employees_delta\"),\n",
        "    \"Delta Lake\",\n",
        "    \"Complex Analytics Query\"\n",
        ")\n",
        "\n",
        "# Test Iceberg performance\n",
        "iceberg_time = measure_query_performance(\n",
        "    complex_query.format(table=\"iceberg.lasagna_demo.employees_iceberg\"),\n",
        "    \"Iceberg\",\n",
        "    \"Complex Analytics Query\"\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Performance Summary:\")\n",
        "if hive_time:\n",
        "    print(f\"  Hive: {hive_time:.2f}s\")\n",
        "if delta_time:\n",
        "    print(f\"  Delta Lake: {delta_time:.2f}s\")\n",
        "if iceberg_time:\n",
        "    print(f\"  Iceberg: {iceberg_time:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Architecture Summary and Verification\n",
        "\n",
        "Let's verify all components are working and summarize what we've tested:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final verification and summary\n",
        "print(\"üéØ LASAGNA Architecture Verification Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check all tables exist\n",
        "print(\"\\nüìã Table Verification:\")\n",
        "tables_to_check = [\n",
        "    (\"employees_hive\", \"Hive\"),\n",
        "    (\"sales_hive\", \"Hive\"),\n",
        "    (\"employees_delta\", \"Delta Lake\"),\n",
        "    (\"sales_delta\", \"Delta Lake\"),\n",
        "    (\"iceberg.lasagna_demo.employees_iceberg\", \"Iceberg\")\n",
        "]\n",
        "\n",
        "for table, format_type in tables_to_check:\n",
        "    try:\n",
        "        count = spark.sql(f\"SELECT COUNT(*) FROM {table}\").collect()[0][0]\n",
        "        print(f\"‚úÖ {format_type}: {table} - {count:,} records\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {format_type}: {table} - Error: {e}\")\n",
        "\n",
        "# Check MinIO storage\n",
        "print(\"\\nüóÑÔ∏è Storage Verification:\")\n",
        "try:\n",
        "    # Check if we can read from S3\n",
        "    spark.sql(\"SELECT COUNT(*) FROM employees_hive\").collect()\n",
        "    print(\"‚úÖ MinIO S3 Storage: Accessible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå MinIO S3 Storage: Error - {e}\")\n",
        "\n",
        "# Check Hive Metastore\n",
        "print(\"\\nüìä Metadata Verification:\")\n",
        "try:\n",
        "    databases = spark.sql(\"SHOW DATABASES\").collect()\n",
        "    print(f\"‚úÖ Hive Metastore: {len(databases)} databases found\")\n",
        "    print(f\"   Databases: {[db[0] for db in databases]}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Hive Metastore: Error - {e}\")\n",
        "\n",
        "# Check Spark Cluster\n",
        "print(\"\\n‚ö° Spark Cluster Status:\")\n",
        "try:\n",
        "    print(f\"‚úÖ Spark Master: {spark.conf.get('spark.master')}\")\n",
        "    print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "    print(f\"‚úÖ Executor Instances: {spark.conf.get('spark.executor.instances')}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Spark Cluster: Error - {e}\")\n",
        "\n",
        "print(\"\\nüéâ Architecture Test Complete!\")\n",
        "print(\"\\nüìù What We Successfully Tested:\")\n",
        "print(\"   ‚úÖ MinIO object storage integration\")\n",
        "print(\"   ‚úÖ PostgreSQL metadata persistence\")\n",
        "print(\"   ‚úÖ Hive Metastore catalog service\")\n",
        "print(\"   ‚úÖ Spark distributed processing\")\n",
        "print(\"   ‚úÖ Hive table operations\")\n",
        "print(\"   ‚úÖ Delta Lake ACID transactions and time travel\")\n",
        "print(\"   ‚úÖ Iceberg schema evolution and partitioning\")\n",
        "print(\"   ‚úÖ Trino cross-engine querying\")\n",
        "print(\"   ‚úÖ Performance comparisons\")\n",
        "print(\"\\nüöÄ Your LASAGNA big data architecture is fully functional!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
