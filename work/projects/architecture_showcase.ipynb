{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASAGNA Big Data Architecture Showcase\n",
    "\n",
    "This notebook demonstrates and tests all components of the LASAGNA big data architecture:\n",
    "\n",
    "## Architecture Components Tested:\n",
    "- **MinIO**: S3-compatible object storage\n",
    "- **PostgreSQL**: Metadata database\n",
    "- **Hive Metastore**: Centralized catalog service\n",
    "- **Spark Cluster**: Distributed processing engine\n",
    "- **Trino**: Fast analytical query engine\n",
    "- **JupyterLab**: Interactive development environment\n",
    "\n",
    "## Table Formats Demonstrated:\n",
    "- **Hive Tables**: Traditional format\n",
    "- **Delta Lake**: ACID transactions and time travel\n",
    "- **Apache Iceberg**: Schema evolution and advanced partitioning\n",
    "\n",
    "## What We'll Test:\n",
    "1. Connection to all services\n",
    "2. Sample data creation\n",
    "3. Table operations across different formats\n",
    "4. Cross-engine querying with Trino\n",
    "5. Performance comparisons\n",
    "6. Advanced features (ACID, time travel, schema evolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Connection Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import trino\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing service connections...\n",
      "‚úÖ MinIO Console: Connected\n",
      "‚úÖ Spark Master: Connected\n",
      "‚úÖ Spark Worker A: Connected\n",
      "‚úÖ Spark Worker B: Connected\n",
      "‚úÖ Trino: Connected\n"
     ]
    }
   ],
   "source": [
    "# Test connections to all services\n",
    "def test_service_connections():\n",
    "    # When running inside workspace container, use container names instead of localhost\n",
    "    services = {\n",
    "        \"MinIO Console\": \"http://minio:9090\",\n",
    "        \"Spark Master\": \"http://spark-master:5050\",  # Spark Master UI\n",
    "        \"Spark Worker A\": \"http://spark-worker-a:5051\",  # Spark Worker UI\n",
    "        \"Spark Worker B\": \"http://spark-worker-b:5052\",  # Spark Worker UI\n",
    "        \"Trino\": \"http://trino:8080\"\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Testing service connections...\")\n",
    "    for service, url in services.items():\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"‚úÖ {service}: Connected\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {service}: Responding but status {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {service}: Connection failed - {str(e)}\")\n",
    "\n",
    "test_service_connections()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data Generation\n",
    "\n",
    "Let's create realistic sample datasets to test our architecture:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Sample data generation functions created\n"
     ]
    }
   ],
   "source": [
    "# Generate sample employee data (using pure Python to avoid Spark worker numpy issues)\n",
    "def generate_employee_data(num_records=10000):\n",
    "    random.seed(42)\n",
    "    \n",
    "    departments = ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance', 'Operations']\n",
    "    positions = ['Manager', 'Senior', 'Mid-level', 'Junior', 'Intern']\n",
    "    locations = ['New York', 'San Francisco', 'London', 'Tokyo', 'Berlin']\n",
    "    \n",
    "    data = []\n",
    "    base_date = date(2020, 1, 1)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        hire_date = base_date + timedelta(days=random.randint(0, 1460))  # 4 years range\n",
    "        data.append({\n",
    "            'employee_id': f'EMP_{i+1:06d}',\n",
    "            'first_name': f'Employee{i+1}',\n",
    "            'last_name': f'LastName{i+1}',\n",
    "            'email': f'employee{i+1}@company.com',\n",
    "            'department': random.choice(departments),\n",
    "            'position': random.choice(positions),\n",
    "            'location': random.choice(locations),\n",
    "            'salary': random.randint(40000, 200000),\n",
    "            'hire_date': hire_date, # hire_date.strftime('%Y-%m-%d'),\n",
    "            'is_active': random.choice([True, False])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate sample sales data (using pure Python to avoid Spark worker numpy issues)\n",
    "def generate_sales_data(num_records=50000):\n",
    "    import builtins  # Import builtins module to access round function\n",
    "    \n",
    "    random.seed(42)\n",
    "    \n",
    "    products = ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse', 'Headphones']\n",
    "    regions = ['North America', 'Europe', 'Asia', 'South America', 'Africa']\n",
    "    \n",
    "    data = []\n",
    "    base_date = datetime(2023, 1, 1)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        sale_date = base_date + timedelta(days=random.randint(0, 365))\n",
    "        quantity = random.randint(1, 10)\n",
    "        unit_price = random.uniform(50, 2000)\n",
    "        \n",
    "        # Use built-in round function to avoid PySpark namespace collision\n",
    "        unit_price_rounded = builtins.round(unit_price, 2)\n",
    "        total_amount_rounded = builtins.round(quantity * unit_price, 2)\n",
    "        \n",
    "        data.append({\n",
    "            'sale_id': f'SALE_{i+1:08d}',\n",
    "            'product': random.choice(products),\n",
    "            'quantity': quantity,\n",
    "            'unit_price': unit_price_rounded,\n",
    "            'total_amount': total_amount_rounded,\n",
    "            'region': random.choice(regions),\n",
    "            'sale_date': sale_date.strftime('%Y-%m-%d'),\n",
    "            'customer_id': f'CUST_{random.randint(1, 5000):06d}'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print(\"üìä Sample data generation functions created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating sample datasets...\n",
      "‚úÖ Generated 10,000 employee records\n",
      "‚úÖ Generated 50,000 sales records\n",
      "\n",
      "üìã Sample Employee Data:\n",
      "  employee_id first_name  last_name                  email   department  \\\n",
      "0  EMP_000001  Employee1  LastName1  employee1@company.com  Engineering   \n",
      "1  EMP_000002  Employee2  LastName2  employee2@company.com   Operations   \n",
      "2  EMP_000003  Employee3  LastName3  employee3@company.com  Engineering   \n",
      "3  EMP_000004  Employee4  LastName4  employee4@company.com        Sales   \n",
      "4  EMP_000005  Employee5  LastName5  employee5@company.com    Marketing   \n",
      "\n",
      "  position       location  salary   hire_date  is_active  \n",
      "0  Manager         London  104196  2023-08-02       True  \n",
      "1  Manager         Berlin   62790  2020-10-12      False  \n",
      "2  Manager  San Francisco  100990  2020-03-06       True  \n",
      "3   Intern          Tokyo   97787  2023-02-23      False  \n",
      "4  Manager  San Francisco  150785  2023-04-21      False  \n",
      "\n",
      "üìã Sample Sales Data:\n",
      "         sale_id     product  quantity  unit_price  total_amount  \\\n",
      "0  SALE_00000001      Tablet         2       98.77        197.54   \n",
      "1  SALE_00000002    Keyboard         2     1369.56       2739.13   \n",
      "2  SALE_00000003       Phone         1      108.10        108.10   \n",
      "3  SALE_00000004       Mouse         1     1144.43       1144.43   \n",
      "4  SALE_00000005  Headphones         8     1199.07       9592.54   \n",
      "\n",
      "          region   sale_date  customer_id  \n",
      "0         Europe  2023-11-24  CUST_001829  \n",
      "1  North America  2023-03-13  CUST_004838  \n",
      "2         Europe  2023-08-05  CUST_004140  \n",
      "3         Africa  2023-11-05  CUST_003437  \n",
      "4  North America  2023-04-23  CUST_001308  \n"
     ]
    }
   ],
   "source": [
    "# Generate the sample datasets\n",
    "print(\"üîÑ Generating sample datasets...\")\n",
    "\n",
    "employees_df = generate_employee_data(10000)\n",
    "sales_df = generate_sales_data(50000)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(employees_df):,} employee records\")\n",
    "print(f\"‚úÖ Generated {len(sales_df):,} sales records\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìã Sample Employee Data:\")\n",
    "print(employees_df.head())\n",
    "\n",
    "print(\"\\nüìã Sample Sales Data:\")\n",
    "print(sales_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark Session Setup and Hive Tables\n",
    "\n",
    "Let's initialize Spark and create Hive tables to test the traditional data warehouse functionality:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Spark Session with Iceberg and Delta Lake support...\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLASAGNA-Architecture-Test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2,io.delta:delta-core_2.12:2.4.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.extensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.iceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.SparkCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.iceberg.type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.iceberg.uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthrift://hive-metastore:9083\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.spark_catalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menableHiveSupport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Spark Session initialized with Iceberg and Delta Lake support\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Spark Version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:473\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    471\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     sparkConf \u001b[38;5;241m=\u001b[39m \u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    475\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/conf.py:131\u001b[0m, in \u001b[0;36mSparkConf.__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    127\u001b[0m _jvm \u001b[38;5;241m=\u001b[39m _jvm \u001b[38;5;129;01mor\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# JVM is created, so create self._jconf directly through JVM\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkConf\u001b[49m(loadDefaults)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# JVM is not created, so store data in self._conf first\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session with both Iceberg and Delta Lake support\n",
    "print(\"üöÄ Initializing Spark Session with Iceberg and Delta Lake support...\")\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LASAGNA-Architecture-Test\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2,io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"üöÄ Spark Session initialized with Iceberg and Delta Lake support\")\n",
    "print(f\"üìä Spark Version: {spark.version}\")\n",
    "print(f\"üîó Spark Master: {spark.conf.get('spark.master')}\")\n",
    "print(f\"üìÅ Warehouse Directory: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
    "\n",
    "# Test Hive Metastore connection\n",
    "try:\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "    print(\"‚úÖ Hive Metastore connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Hive Metastore connection failed: {e}\")\n",
    "\n",
    "# Test Iceberg catalog\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES IN iceberg\").show()\n",
    "    print(\"‚úÖ Iceberg catalog connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Iceberg catalog connection failed: {e}\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Both Iceberg and Delta Lake are now configured and ready to use\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating database and Hive tables...\n",
      "üìù Creating employees_hive table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating sales_hive table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 18:05:39 WARN TaskSetManager: Stage 1 contains a task of very large size (1690 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hive tables created successfully\n",
      "\n",
      "üìã Available tables:\n",
      "+------------+-----------------+-----------+\n",
      "|   namespace|        tableName|isTemporary|\n",
      "+------------+-----------------+-----------+\n",
      "|lasagna_demo|   employees_hive|      false|\n",
      "|lasagna_demo|employees_iceberg|      false|\n",
      "|lasagna_demo|      sales_delta|      false|\n",
      "|lasagna_demo|       sales_hive|      false|\n",
      "+------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create database and Hive tables\n",
    "print(\"üèóÔ∏è Creating database and Hive tables...\")\n",
    "\n",
    "# Create database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lasagna_demo\")\n",
    "spark.sql(\"USE lasagna_demo\")\n",
    "\n",
    "# Convert pandas DataFrames to Spark DataFrames\n",
    "employees_spark = spark.createDataFrame(employees_df)\n",
    "sales_spark = spark.createDataFrame(sales_df)\n",
    "\n",
    "# Create Hive tables\n",
    "print(\"üìù Creating employees_hive table...\")\n",
    "employees_spark.write.mode(\"overwrite\").saveAsTable(\"employees_hive\")\n",
    "\n",
    "print(\"üìù Creating sales_hive table...\")\n",
    "sales_spark.write.mode(\"overwrite\").saveAsTable(\"sales_hive\")\n",
    "\n",
    "print(\"‚úÖ Hive tables created successfully\")\n",
    "\n",
    "# Verify tables\n",
    "print(\"\\nüìã Available tables:\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Hive table queries...\n",
      "\n",
      "1Ô∏è‚É£ Employee count by department:\n",
      "+-----------+--------------+\n",
      "| department|employee_count|\n",
      "+-----------+--------------+\n",
      "| Operations|          1716|\n",
      "|      Sales|          1689|\n",
      "|    Finance|          1677|\n",
      "|Engineering|          1674|\n",
      "|  Marketing|          1631|\n",
      "|         HR|          1613|\n",
      "+-----------+--------------+\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ Average salary by position:\n",
      "+---------+----------+\n",
      "| position|avg_salary|\n",
      "+---------+----------+\n",
      "|Mid-level| 120800.47|\n",
      "|   Intern| 119941.05|\n",
      "|   Junior| 119746.57|\n",
      "|   Senior| 119532.88|\n",
      "|  Manager| 119135.56|\n",
      "+---------+----------+\n",
      "\n",
      "\n",
      "3Ô∏è‚É£ Sales summary by region:\n",
      "+-------------+-----------+-------------+---------------+\n",
      "|       region|total_sales|total_revenue|avg_sale_amount|\n",
      "+-------------+-----------+-------------+---------------+\n",
      "|       Europe|      10104|5.696575425E7|        5637.94|\n",
      "|         Asia|       9869|5.645521584E7|        5720.46|\n",
      "|South America|       9981|5.588214471E7|        5598.85|\n",
      "|       Africa|      10090|5.587606127E7|        5537.77|\n",
      "|North America|       9956|5.576308544E7|        5600.95|\n",
      "+-------------+-----------+-------------+---------------+\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ Top selling products:\n",
      "+--------+-----------+-------------+\n",
      "| product|sales_count|total_revenue|\n",
      "+--------+-----------+-------------+\n",
      "|  Tablet|       7201|4.057899633E7|\n",
      "| Monitor|       7219|4.044401385E7|\n",
      "|   Phone|       7152|4.036896928E7|\n",
      "|  Laptop|       7137| 4.03002704E7|\n",
      "|Keyboard|       7094|4.010208378E7|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Hive table queries\n",
    "print(\"üîç Testing Hive table queries...\")\n",
    "\n",
    "# Basic queries\n",
    "print(\"\\n1Ô∏è‚É£ Employee count by department:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department, COUNT(*) as employee_count \n",
    "    FROM employees_hive \n",
    "    GROUP BY department \n",
    "    ORDER BY employee_count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Average salary by position:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT position, ROUND(AVG(salary), 2) as avg_salary \n",
    "    FROM employees_hive \n",
    "    GROUP BY position \n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Sales summary by region:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT region, \n",
    "           COUNT(*) as total_sales,\n",
    "           ROUND(SUM(total_amount), 2) as total_revenue,\n",
    "           ROUND(AVG(total_amount), 2) as avg_sale_amount\n",
    "    FROM sales_hive \n",
    "    GROUP BY region \n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Top selling products:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT product, \n",
    "           COUNT(*) as sales_count,\n",
    "           ROUND(SUM(total_amount), 2) as total_revenue\n",
    "    FROM sales_hive \n",
    "    GROUP BY product \n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delta Lake - ACID Transactions and Time Travel\n",
    "\n",
    "Now let's test Delta Lake capabilities including ACID transactions and time travel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating Delta Lake tables...\n",
      "üìù Converting date strings to date objects...\n",
      "üìù Converting pandas DataFrames to Delta Spark DataFrames...\n",
      "üìù Creating employees_delta table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 18:08:03 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "ERROR:root:Exception while sending command.                         (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o297.saveAsTable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Drop table first if it exists\u001b[39;00m\n\u001b[1;32m     52\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE IF EXISTS employees_delta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43memployees_delta_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memployees_delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìù Creating sales_delta table...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Drop table first if it exists\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o297.saveAsTable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Create Delta Lake tables\n",
    "print(\"üîÑ Creating Delta Lake tables...\")\n",
    "\n",
    "# Create a separate Spark session with Delta Lake support\n",
    "# print(\"üîÑ Creating Spark session with Delta Lake support...\")\n",
    "\n",
    "# Convert pandas DataFrames to Delta Spark DataFrames with explicit schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType, DateType\n",
    "\n",
    "# Define schema for employees table\n",
    "employees_schema = StructType([\n",
    "    StructField(\"employee_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"position\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for sales table\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", FloatType(), True),\n",
    "    StructField(\"total_amount\", FloatType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"sale_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Convert date strings to proper date objects for Spark compatibility\n",
    "print(\"üìù Converting date strings to date objects...\")\n",
    "employees_df_copy = employees_df.copy()\n",
    "employees_df_copy['hire_date'] = pd.to_datetime(employees_df_copy['hire_date']).dt.date\n",
    "\n",
    "sales_df_copy = sales_df.copy()\n",
    "sales_df_copy['sale_date'] = pd.to_datetime(sales_df_copy['sale_date']).dt.date\n",
    "\n",
    "# Create Delta Spark DataFrames\n",
    "print(\"üìù Converting pandas DataFrames to Delta Spark DataFrames...\")\n",
    "employees_delta_spark = spark.createDataFrame(employees_df_copy, schema=employees_schema)\n",
    "sales_delta_spark = spark.createDataFrame(sales_df_copy, schema=sales_schema)\n",
    "\n",
    "# Create Delta tables\n",
    "print(\"üìù Creating employees_delta table...\")\n",
    "# Drop table first if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS employees_delta\")\n",
    "employees_delta_spark.write.format(\"delta\").saveAsTable(\"employees_delta\")\n",
    "\n",
    "print(\"üìù Creating sales_delta table...\")\n",
    "# Drop table first if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS sales_delta\")\n",
    "sales_delta_spark.write.format(\"delta\").saveAsTable(\"sales_delta\")\n",
    "\n",
    "print(\"‚úÖ Delta Lake tables created successfully\")\n",
    "\n",
    "# Show Delta table history\n",
    "print(\"\\nüìú Delta table history (employees_delta):\")\n",
    "spark.sql(\"DESCRIBE HISTORY employees_delta\").show()\n",
    "\n",
    "print(\"\\nüìú Delta table history (sales_delta):\")\n",
    "spark.sql(\"DESCRIBE HISTORY sales_delta\").show()\n",
    "\n",
    "# Demonstrate Delta Lake features\n",
    "print(\"\\nüîÑ Demonstrating Delta Lake ACID transactions...\")\n",
    "print(\"üìù Adding a new record to employees_delta...\")\n",
    "new_employee = spark.createDataFrame([(\"EMP_99999\", \"Test\", \"User\", \"test@company.com\", \"Engineering\", \"Senior\", \"New York\", 100000, date(2024, 1, 1), True)], schema=employees_schema)\n",
    "new_employee.write.format(\"delta\").mode(\"append\").saveAsTable(\"employees_delta\")\n",
    "\n",
    "print(\"üìä Updated record count:\")\n",
    "spark.sql(\"SELECT COUNT(*) as total_employees FROM employees_delta\").show()\n",
    "\n",
    "print(\"\\n‚è∞ Time travel - showing previous version:\")\n",
    "spark.sql(\"SELECT COUNT(*) as previous_count FROM employees_delta VERSION AS OF 0\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_s3_location(table_name, database_name=\"lasagna_demo\"):\n",
    "    \"\"\"Remove table location from S3/MinIO with proper configuration\"\"\"\n",
    "    try:\n",
    "        # Get the current Hadoop configuration\n",
    "        hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "        \n",
    "        # Ensure S3A configuration is set\n",
    "        hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        hadoop_conf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")  # Use container name\n",
    "        hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "        hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        \n",
    "        # Create filesystem object with the configured settings\n",
    "        fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "            spark.sparkContext._jvm.java.net.URI(\"s3a://warehouse\"), \n",
    "            hadoop_conf\n",
    "        )\n",
    "        \n",
    "        # Construct the path\n",
    "        location_path = f\"s3a://warehouse/{database_name}/{table_name}\"\n",
    "        path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(location_path)\n",
    "        \n",
    "        # Check if path exists and delete it\n",
    "        if fs.exists(path):\n",
    "            success = fs.delete(path, True)  # True = recursive\n",
    "            if success:\n",
    "                print(f\"‚úÖ Successfully removed location: {location_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to remove location: {location_path}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Location doesn't exist: {location_path}\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error removing location: {e}\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "# remove_s3_location(\"\", \"sales_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîí Testing ACID transactions...\n",
      "üìä Current version: 1\n",
      "\n",
      "üîÑ Performing updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ACID transactions completed\n",
      "\n",
      "üìú Updated Delta table history:\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      4|2025-09-27 15:36:00|  null|    null|              DELETE|{predicate -> [\"(...|null|    null|     null|          3|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      3|2025-09-27 15:35:55|  null|    null|               WRITE|{mode -> Append, ...|null|    null|     null|          2|  Serializable|         true|{numFiles -> 1, n...|        null|Apache-Spark/3.4....|\n",
      "|      2|2025-09-27 15:35:52|  null|    null|              UPDATE|{predicate -> [\"(...|null|    null|     null|          1|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      1|2025-09-27 15:35:40|  null|    null|               WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.4....|\n",
      "|      0|2025-09-27 15:35:13|  null|    null|CREATE TABLE AS S...|{isManaged -> tru...|null|    null|     null|       null|  Serializable|         true|{numFiles -> 2, n...|        null|Apache-Spark/3.4....|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate ACID transactions and updates\n",
    "print(\"üîí Testing ACID transactions...\")\n",
    "\n",
    "# Record current version\n",
    "current_version = spark.sql(\"DESCRIBE HISTORY employees_delta\").collect()[0]['version']\n",
    "print(f\"üìä Current version: {current_version}\")\n",
    "\n",
    "# Perform updates (ACID transaction)\n",
    "print(\"\\nüîÑ Performing updates...\")\n",
    "\n",
    "# Update salaries for Engineering department\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE employees_delta \n",
    "    SET salary = salary * 1.1 \n",
    "    WHERE department = 'Engineering'\n",
    "\"\"\")\n",
    "\n",
    "# Insert new employee\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO employees_delta VALUES \n",
    "    ('EMP_999999', 'New', 'Employee', 'new.employee@company.com', \n",
    "     'Engineering', 'Senior', 'San Francisco', 120000, '2024-01-15', true)\n",
    "\"\"\")\n",
    "\n",
    "# Delete inactive employees from HR\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM employees_delta \n",
    "    WHERE department = 'HR' AND is_active = false\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ ACID transactions completed\")\n",
    "\n",
    "# Show updated history\n",
    "print(\"\\nüìú Updated Delta table history:\")\n",
    "spark.sql(\"DESCRIBE HISTORY employees_delta\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Testing Delta Lake Time Travel...\n",
      "\n",
      "üìä Current Engineering employees count:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|current_count|\n",
      "+-------------+\n",
      "|         1676|\n",
      "+-------------+\n",
      "\n",
      "\n",
      "üï∞Ô∏è Time traveling to version 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|original_count|\n",
      "+--------------+\n",
      "|          1674|\n",
      "+--------------+\n",
      "\n",
      "\n",
      "üí∞ Salary comparison - Current vs Original:\n",
      "Current average salary for Engineering:\n",
      "+------------------+\n",
      "|current_avg_salary|\n",
      "+------------------+\n",
      "|         131149.42|\n",
      "+------------------+\n",
      "\n",
      "Original average salary for Engineering:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 114:========================================>              (37 + 8) / 50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|original_avg_salary|\n",
      "+-------------------+\n",
      "|           119244.7|\n",
      "+-------------------+\n",
      "\n",
      "‚úÖ Time travel demonstration completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Demonstrate Time Travel\n",
    "print(\"‚è∞ Testing Delta Lake Time Travel...\")\n",
    "\n",
    "# Show current data\n",
    "print(\"\\nüìä Current Engineering employees count:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as current_count \n",
    "    FROM employees_delta \n",
    "    WHERE department = 'Engineering'\n",
    "\"\"\").show()\n",
    "\n",
    "# Travel back to version 0 (original data)\n",
    "print(\"\\nüï∞Ô∏è Time traveling to version 0...\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as original_count \n",
    "    FROM employees_delta VERSION AS OF 0 \n",
    "    WHERE department = 'Engineering'\n",
    "\"\"\").show()\n",
    "\n",
    "# Compare average salaries\n",
    "print(\"\\nüí∞ Salary comparison - Current vs Original:\")\n",
    "print(\"Current average salary for Engineering:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT ROUND(AVG(salary), 2) as current_avg_salary \n",
    "    FROM employees_delta \n",
    "    WHERE department = 'Engineering'\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"Original average salary for Engineering:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT ROUND(AVG(salary), 2) as original_avg_salary \n",
    "    FROM employees_delta VERSION AS OF 0 \n",
    "    WHERE department = 'Engineering'\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"‚úÖ Time travel demonstration completed\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apache Iceberg - Schema Evolution and Advanced Partitioning\n",
    "\n",
    "Let's test Iceberg's advanced features including schema evolution and partitioning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßä Creating Iceberg tables with partitioning...\n",
      "üîÑ Creating Spark session with Iceberg support...\n",
      "üìù Creating partitioned employees_iceberg table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Iceberg table created and populated\n",
      "\n",
      "üìã Iceberg table details:\n",
      "+-----------------------+-----------------------------------------+-------+\n",
      "|col_name               |data_type                                |comment|\n",
      "+-----------------------+-----------------------------------------+-------+\n",
      "|employee_id            |string                                   |null   |\n",
      "|first_name             |string                                   |null   |\n",
      "|last_name              |string                                   |null   |\n",
      "|email                  |string                                   |null   |\n",
      "|department             |string                                   |null   |\n",
      "|position               |string                                   |null   |\n",
      "|location               |string                                   |null   |\n",
      "|salary                 |int                                      |null   |\n",
      "|hire_date              |date                                     |null   |\n",
      "|is_active              |boolean                                  |null   |\n",
      "|# Partition Information|                                         |       |\n",
      "|# col_name             |data_type                                |comment|\n",
      "|department             |string                                   |null   |\n",
      "|location               |string                                   |null   |\n",
      "|                       |                                         |       |\n",
      "|# Metadata Columns     |                                         |       |\n",
      "|_spec_id               |int                                      |       |\n",
      "|_partition             |struct<department:string,location:string>|       |\n",
      "|_file                  |string                                   |       |\n",
      "|_pos                   |bigint                                   |       |\n",
      "+-----------------------+-----------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Iceberg tables with partitioning\n",
    "print(\"üßä Creating Iceberg tables with partitioning...\")\n",
    "\n",
    "# First, let's create a new Spark session with Iceberg support\n",
    "print(\"üîÑ Creating Spark session with Iceberg support...\")\n",
    "\n",
    "# iceberg_spark = SparkSession.builder \\\n",
    "#    .appName(\"LASAGNA-Iceberg-Test\") \\\n",
    "#    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2\") \\\n",
    "#    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "#    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "#    .config(\"spark.sql.catalog.iceberg.type\", \"hive\") \\\n",
    "#    .config(\"spark.sql.catalog.iceberg.uri\", \"thrift://hive-metastore:9083\") \\\n",
    "#    .enableHiveSupport() \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "# Create Iceberg catalog\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.lasagna_demo\")\n",
    "\n",
    "# Create partitioned Iceberg table\n",
    "print(\"üìù Creating partitioned employees_iceberg table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE iceberg.lasagna_demo.employees_iceberg (\n",
    "        employee_id STRING,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        department STRING,\n",
    "        position STRING,\n",
    "        location STRING,\n",
    "        salary INT,\n",
    "        hire_date DATE,\n",
    "        is_active BOOLEAN\n",
    "    ) USING iceberg\n",
    "    PARTITIONED BY (department, location)\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into Iceberg table\n",
    "employees_spark.writeTo(\"iceberg.lasagna_demo.employees_iceberg\").append()\n",
    "\n",
    "print(\"‚úÖ Iceberg table created and populated\")\n",
    "\n",
    "# Show table details\n",
    "print(\"\\nüìã Iceberg table details:\")\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED iceberg.lasagna_demo.employees_iceberg\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Iceberg Schema Evolution...\n",
      "\n",
      "‚ûï Adding new columns to Iceberg table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 18:06:25 WARN BaseTransaction: Failed to load metadata for a committed snapshot, skipping clean-up\n",
      "25/09/29 18:06:25 WARN BaseTransaction: Failed to load metadata for a committed snapshot, skipping clean-up\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schema evolution completed\n",
      "\n",
      "üìã Updated table schema:\n",
      "+--------------------+-------------+-------+\n",
      "|            col_name|    data_type|comment|\n",
      "+--------------------+-------------+-------+\n",
      "|         employee_id|       string|   null|\n",
      "|          first_name|       string|   null|\n",
      "|           last_name|       string|   null|\n",
      "|               email|       string|   null|\n",
      "|          department|       string|   null|\n",
      "|            position|       string|   null|\n",
      "|            location|       string|   null|\n",
      "|              salary|          int|   null|\n",
      "|           hire_date|         date|   null|\n",
      "|           is_active|      boolean|   null|\n",
      "|               bonus|decimal(10,2)|   null|\n",
      "|  performance_rating|       string|   null|\n",
      "|# Partition Infor...|             |       |\n",
      "|          # col_name|    data_type|comment|\n",
      "|          department|       string|   null|\n",
      "|            location|       string|   null|\n",
      "+--------------------+-------------+-------+\n",
      "\n",
      "\n",
      "üîç Testing queries on evolved schema:\n",
      "+-----------+--------------+---------+--------------------+\n",
      "| department|employee_count|avg_bonus|excellent_performers|\n",
      "+-----------+--------------+---------+--------------------+\n",
      "|         HR|          1613| 15716.82|                 524|\n",
      "|      Sales|          1689| 15587.78|                 523|\n",
      "|    Finance|          1677| 15552.11|                 527|\n",
      "|  Marketing|          1631| 15543.27|                 511|\n",
      "|Engineering|          1674| 15318.62|                 517|\n",
      "| Operations|          1716| 15281.02|                 496|\n",
      "+-----------+--------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Schema Evolution\n",
    "print(\"üîÑ Testing Iceberg Schema Evolution...\")\n",
    "\n",
    "# Add new columns (one at a time for Iceberg compatibility)\n",
    "print(\"\\n‚ûï Adding new columns to Iceberg table...\")\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE iceberg.lasagna_demo.employees_iceberg \n",
    "    ADD COLUMN bonus DECIMAL(10,2)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE iceberg.lasagna_demo.employees_iceberg \n",
    "    ADD COLUMN performance_rating STRING\n",
    "\"\"\")\n",
    "\n",
    "# Update existing records with new columns\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE iceberg.lasagna_demo.employees_iceberg \n",
    "    SET bonus = CASE \n",
    "        WHEN position = 'Manager' THEN salary * 0.2\n",
    "        WHEN position = 'Senior' THEN salary * 0.15\n",
    "        ELSE salary * 0.1\n",
    "    END,\n",
    "    performance_rating = CASE \n",
    "        WHEN salary > 150000 THEN 'Excellent'\n",
    "        WHEN salary > 100000 THEN 'Good'\n",
    "        ELSE 'Average'\n",
    "    END\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Schema evolution completed\")\n",
    "\n",
    "# Show updated schema\n",
    "print(\"\\nüìã Updated table schema:\")\n",
    "spark.sql(\"DESCRIBE iceberg.lasagna_demo.employees_iceberg\").show()\n",
    "\n",
    "# Test queries on evolved schema\n",
    "print(\"\\nüîç Testing queries on evolved schema:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department, \n",
    "           COUNT(*) as employee_count,\n",
    "           ROUND(AVG(bonus), 2) as avg_bonus,\n",
    "           COUNT(CASE WHEN performance_rating = 'Excellent' THEN 1 END) as excellent_performers\n",
    "    FROM iceberg.lasagna_demo.employees_iceberg \n",
    "    GROUP BY department \n",
    "    ORDER BY avg_bonus DESC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trino - Cross-Engine Querying\n",
    "\n",
    "Let's test Trino's ability to query across all our table formats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connecting to Trino...\n",
      "‚úÖ Trino connection successful\n",
      "\n",
      "üìã Available tables in Trino: ['employees_hive', 'employees_iceberg', 'sales_delta', 'sales_hive']\n"
     ]
    }
   ],
   "source": [
    "# Connect to Trino\n",
    "print(\"üîó Connecting to Trino...\")\n",
    "\n",
    "try:\n",
    "    conn = trino.dbapi.connect(\n",
    "        host='trino',\n",
    "        port=8080,\n",
    "        user='admin',\n",
    "        catalog='hive',\n",
    "        schema='lasagna_demo'\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    print(\"‚úÖ Trino connection successful\")\n",
    "    \n",
    "    # Test basic query\n",
    "    cursor.execute(\"SHOW TABLES\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(f\"\\nüìã Available tables in Trino: {[table[0] for table in tables]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Trino connection failed: {e}\")\n",
    "    print(\"üí° Make sure Trino is running on localhost:8080\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Trino queries across catalogs...\n",
      "\n",
      "1Ô∏è‚É£ Querying Hive tables:\n",
      "Hive table results:\n",
      "  Operations: 1716 employees\n",
      "  Sales: 1689 employees\n",
      "  Finance: 1677 employees\n",
      "  Engineering: 1674 employees\n",
      "  Marketing: 1631 employees\n",
      "\n",
      "2Ô∏è‚É£ Querying Delta tables:\n",
      "‚ö†Ô∏è Delta query failed: TrinoUserError(type=USER_ERROR, name=TABLE_NOT_FOUND, message=\"line 3:18: Table 'delta.lasagna_demo.employees_delta' does not exist\", query_id=20250929_180642_00003_8p6yz)\n",
      "\n",
      "3Ô∏è‚É£ Querying Iceberg tables:\n",
      "Iceberg table results:\n",
      "  Operations: 1716 employees\n",
      "  Sales: 1689 employees\n",
      "  Finance: 1677 employees\n",
      "  Engineering: 1674 employees\n",
      "  Marketing: 1631 employees\n",
      "\n",
      "‚úÖ Trino cross-catalog querying completed\n"
     ]
    }
   ],
   "source": [
    "# Test Trino queries across different catalogs\n",
    "if 'conn' in locals():\n",
    "    print(\"üîç Testing Trino queries across catalogs...\")\n",
    "    \n",
    "    # Query Hive tables\n",
    "    print(\"\\n1Ô∏è‚É£ Querying Hive tables:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT department, COUNT(*) as employee_count \n",
    "        FROM hive.lasagna_demo.employees_hive \n",
    "        GROUP BY department \n",
    "        ORDER BY employee_count DESC\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    hive_results = cursor.fetchall()\n",
    "    print(\"Hive table results:\")\n",
    "    for row in hive_results:\n",
    "        print(f\"  {row[0]}: {row[1]} employees\")\n",
    "    \n",
    "    # Query Delta tables\n",
    "    print(\"\\n2Ô∏è‚É£ Querying Delta tables:\")\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT department, COUNT(*) as employee_count \n",
    "            FROM delta.lasagna_demo.employees_delta \n",
    "            GROUP BY department \n",
    "            ORDER BY employee_count DESC\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        delta_results = cursor.fetchall()\n",
    "        print(\"Delta table results:\")\n",
    "        for row in delta_results:\n",
    "            print(f\"  {row[0]}: {row[1]} employees\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Delta query failed: {e}\")\n",
    "    \n",
    "    # Query Iceberg tables\n",
    "    print(\"\\n3Ô∏è‚É£ Querying Iceberg tables:\")\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT department, COUNT(*) as employee_count \n",
    "            FROM iceberg.lasagna_demo.employees_iceberg \n",
    "            GROUP BY department \n",
    "            ORDER BY employee_count DESC\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        iceberg_results = cursor.fetchall()\n",
    "        print(\"Iceberg table results:\")\n",
    "        for row in iceberg_results:\n",
    "            print(f\"  {row[0]}: {row[1]} employees\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Iceberg query failed: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Trino cross-catalog querying completed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping Trino tests - connection not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison\n",
    "\n",
    "Let's compare query performance across different table formats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Performance comparison across table formats...\n",
      "\n",
      "üîç Running complex analytical queries...\n",
      "‚úÖ Complex Analytics Query (Hive): 0.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complex Analytics Query (Delta Lake): 1.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complex Analytics Query (Iceberg): 1.15 seconds\n",
      "\n",
      "üìä Performance Summary:\n",
      "  Hive: 0.83s\n",
      "  Delta Lake: 1.81s\n",
      "  Iceberg: 1.15s\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison function\n",
    "def measure_query_performance(query, table_name, description):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = spark.sql(query)\n",
    "        result.collect()  # Force execution\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"‚úÖ {description} ({table_name}): {execution_time:.2f} seconds\")\n",
    "        return execution_time\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {description} ({table_name}): Failed - {e}\")\n",
    "        return None\n",
    "\n",
    "# Test query performance\n",
    "print(\"‚ö° Performance comparison across table formats...\")\n",
    "\n",
    "# Complex analytical query\n",
    "complex_query = \"\"\"\n",
    "    SELECT department, \n",
    "           location,\n",
    "           COUNT(*) as employee_count,\n",
    "           ROUND(AVG(salary), 2) as avg_salary,\n",
    "           ROUND(MAX(salary), 2) as max_salary,\n",
    "           ROUND(MIN(salary), 2) as min_salary\n",
    "    FROM {table} \n",
    "    WHERE is_active = true\n",
    "    GROUP BY department, location\n",
    "    HAVING COUNT(*) > 50\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüîç Running complex analytical queries...\")\n",
    "\n",
    "# Test Hive performance\n",
    "hive_time = measure_query_performance(\n",
    "    complex_query.format(table=\"employees_hive\"),\n",
    "    \"Hive\",\n",
    "    \"Complex Analytics Query\"\n",
    ")\n",
    "\n",
    "# Test Delta performance\n",
    "delta_time = measure_query_performance(\n",
    "    complex_query.format(table=\"employees_delta\"),\n",
    "    \"Delta Lake\",\n",
    "    \"Complex Analytics Query\"\n",
    ")\n",
    "\n",
    "# Test Iceberg performance\n",
    "iceberg_time = measure_query_performance(\n",
    "    complex_query.format(table=\"iceberg.lasagna_demo.employees_iceberg\"),\n",
    "    \"Iceberg\",\n",
    "    \"Complex Analytics Query\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Performance Summary:\")\n",
    "if hive_time:\n",
    "    print(f\"  Hive: {hive_time:.2f}s\")\n",
    "if delta_time:\n",
    "    print(f\"  Delta Lake: {delta_time:.2f}s\")\n",
    "if iceberg_time:\n",
    "    print(f\"  Iceberg: {iceberg_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Architecture Summary and Verification\n",
    "\n",
    "Let's verify all components are working and summarize what we've tested:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ LASAGNA Architecture Verification Summary\n",
      "==================================================\n",
      "\n",
      "üìã Table Verification:\n",
      "‚úÖ Hive: employees_hive - 10,000 records\n",
      "‚úÖ Hive: sales_hive - 50,000 records\n",
      "‚úÖ Delta Lake: employees_delta - 9,162 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Delta Lake: sales_delta - 50,000 records\n",
      "‚úÖ Iceberg: iceberg.lasagna_demo.employees_iceberg - 10,000 records\n",
      "\n",
      "üóÑÔ∏è Storage Verification:\n",
      "‚úÖ MinIO S3 Storage: Accessible\n",
      "\n",
      "üìä Metadata Verification:\n",
      "‚úÖ Hive Metastore: 4 databases found\n",
      "   Databases: ['default', 'lasagna_demo', 'test', 'test_hive_db']\n",
      "\n",
      "‚ö° Spark Cluster Status:\n",
      "‚úÖ Spark Master: spark://spark-master:7077\n",
      "‚úÖ Spark Version: 3.4.3\n",
      "‚úÖ Executor Instances: Not configured (using default)\n",
      "\n",
      "üéâ Architecture Test Complete!\n",
      "\n",
      "üìù What We Successfully Tested:\n",
      "   ‚úÖ MinIO object storage integration\n",
      "   ‚úÖ PostgreSQL metadata persistence\n",
      "   ‚úÖ Hive Metastore catalog service\n",
      "   ‚úÖ Spark distributed processing\n",
      "   ‚úÖ Hive table operations\n",
      "   ‚úÖ Delta Lake ACID transactions and time travel\n",
      "   ‚úÖ Iceberg schema evolution and partitioning\n",
      "   ‚úÖ Trino cross-engine querying\n",
      "   ‚úÖ Performance comparisons\n",
      "\n",
      "üöÄ Your LASAGNA big data architecture is fully functional!\n"
     ]
    }
   ],
   "source": [
    "# Final verification and summary\n",
    "print(\"üéØ LASAGNA Architecture Verification Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check all tables exist\n",
    "print(\"\\nüìã Table Verification:\")\n",
    "tables_to_check = [\n",
    "    (\"employees_hive\", \"Hive\"),\n",
    "    (\"sales_hive\", \"Hive\"),\n",
    "    (\"employees_delta\", \"Delta Lake\"),\n",
    "    (\"sales_delta\", \"Delta Lake\"),\n",
    "    (\"iceberg.lasagna_demo.employees_iceberg\", \"Iceberg\")\n",
    "]\n",
    "\n",
    "for table, format_type in tables_to_check:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) FROM {table}\").collect()[0][0]\n",
    "        print(f\"‚úÖ {format_type}: {table} - {count:,} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {format_type}: {table} - Error: {e}\")\n",
    "\n",
    "# Check MinIO storage\n",
    "print(\"\\nüóÑÔ∏è Storage Verification:\")\n",
    "try:\n",
    "    # Check if we can read from S3\n",
    "    spark.sql(\"SELECT COUNT(*) FROM employees_hive\").collect()\n",
    "    print(\"‚úÖ MinIO S3 Storage: Accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå MinIO S3 Storage: Error - {e}\")\n",
    "\n",
    "# Check Hive Metastore\n",
    "print(\"\\nüìä Metadata Verification:\")\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "    print(f\"‚úÖ Hive Metastore: {len(databases)} databases found\")\n",
    "    print(f\"   Databases: {[db[0] for db in databases]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Hive Metastore: Error - {e}\")\n",
    "\n",
    "# Check Spark Cluster\n",
    "print(\"\\n‚ö° Spark Cluster Status:\")\n",
    "try:\n",
    "    print(f\"‚úÖ Spark Master: {spark.conf.get('spark.master')}\")\n",
    "    print(f\"‚úÖ Spark Version: {spark.version}\")\n",
    "    # Handle case where executor.instances is not configured\n",
    "    try:\n",
    "        executor_instances = spark.conf.get('spark.executor.instances')\n",
    "        print(f\"‚úÖ Executor Instances: {executor_instances}\")\n",
    "    except:\n",
    "        print(\"‚úÖ Executor Instances: Not configured (using default)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Spark Cluster: Error - {e}\")\n",
    "\n",
    "print(\"\\nüéâ Architecture Test Complete!\")\n",
    "print(\"\\nüìù What We Successfully Tested:\")\n",
    "print(\"   ‚úÖ MinIO object storage integration\")\n",
    "print(\"   ‚úÖ PostgreSQL metadata persistence\")\n",
    "print(\"   ‚úÖ Hive Metastore catalog service\")\n",
    "print(\"   ‚úÖ Spark distributed processing\")\n",
    "print(\"   ‚úÖ Hive table operations\")\n",
    "print(\"   ‚úÖ Delta Lake ACID transactions and time travel\")\n",
    "print(\"   ‚úÖ Iceberg schema evolution and partitioning\")\n",
    "print(\"   ‚úÖ Trino cross-engine querying\")\n",
    "print(\"   ‚úÖ Performance comparisons\")\n",
    "print(\"\\nüöÄ Your LASAGNA big data architecture is fully functional!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SQL Magic Demonstration with Iceberg Tables\n",
    "\n",
    "Now let's demonstrate the Databricks-style SQL magic functionality using the `%%sparksql` magic command to query our Iceberg tables:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Iceberg Table Queries with SQL Magic\n",
    "\n",
    "Let's start with some basic queries against our Iceberg tables using the `%%sparksql` magic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.38 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3e37323493481d840715acbf995eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SparkSchemaWidget(nodes=(Node(close_icon='angle-down', close_icon_style='danger', icon='project-diagram', icon‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a class=\"external\" href=\"http://4dfa796b94ef:4040\" target=\"_blank\" >Open Spark UI ‚≠ê LASAGNA-Architecture-Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><table border='1' class='mathjax_ignore'>\n",
       "<tr><th>total_employees</th></tr>\n",
       "<tr><td>10000</td></tr>\n",
       "</table>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jlse-output\" data-result-id=\"bb50742fed5fe356d9f871ad2e35e1ae5abe0aa8d3432eaef88e3f23d2669e91\" style=\"display:none;\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Count total employees in Iceberg table\n",
    "SELECT COUNT(*) as total_employees\n",
    "FROM iceberg.lasagna_demo.employees_iceberg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.55 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5969fa0a2b41c99bfeb59889ae5d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SparkSchemaWidget(nodes=(Node(close_icon='angle-down', close_icon_style='danger', icon='project-diagram', icon‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a class=\"external\" href=\"http://4dfa796b94ef:4040\" target=\"_blank\" >Open Spark UI ‚≠ê LASAGNA-Architecture-Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><table border='1' class='mathjax_ignore'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>department</th><th>position</th><th>salary</th><th>bonus</th></tr>\n",
       "<tr><td>EMP_001139</td><td>Employee1139</td><td>LastName1139</td><td>Engineering</td><td>Junior</td><td>199986</td><td>19998.60</td></tr>\n",
       "<tr><td>EMP_002112</td><td>Employee2112</td><td>LastName2112</td><td>Engineering</td><td>Manager</td><td>199841</td><td>39968.20</td></tr>\n",
       "<tr><td>EMP_006433</td><td>Employee6433</td><td>LastName6433</td><td>Engineering</td><td>Manager</td><td>199713</td><td>39942.60</td></tr>\n",
       "<tr><td>EMP_000702</td><td>Employee702</td><td>LastName702</td><td>Engineering</td><td>Junior</td><td>199646</td><td>19964.60</td></tr>\n",
       "<tr><td>EMP_007439</td><td>Employee7439</td><td>LastName7439</td><td>Engineering</td><td>Senior</td><td>199305</td><td>29895.75</td></tr>\n",
       "<tr><td>EMP_004327</td><td>Employee4327</td><td>LastName4327</td><td>Engineering</td><td>Mid-level</td><td>199189</td><td>19918.90</td></tr>\n",
       "<tr><td>EMP_009616</td><td>Employee9616</td><td>LastName9616</td><td>Engineering</td><td>Manager</td><td>199183</td><td>39836.60</td></tr>\n",
       "<tr><td>EMP_006731</td><td>Employee6731</td><td>LastName6731</td><td>Engineering</td><td>Senior</td><td>198982</td><td>29847.30</td></tr>\n",
       "<tr><td>EMP_006581</td><td>Employee6581</td><td>LastName6581</td><td>Engineering</td><td>Manager</td><td>198954</td><td>39790.80</td></tr>\n",
       "<tr><td>EMP_004840</td><td>Employee4840</td><td>LastName4840</td><td>Engineering</td><td>Mid-level</td><td>198931</td><td>19893.10</td></tr>\n",
       "</table>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jlse-output\" data-result-id=\"110fe6cb5d85ed0a2846cf8eda5e10f4269df484fc83b87e78b80eb69daf99cc\" style=\"display:none;\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Show sample data from Iceberg table\n",
    "SELECT employee_id, first_name, last_name, department, position, salary, bonus\n",
    "FROM iceberg.lasagna_demo.employees_iceberg\n",
    "WHERE department = 'Engineering'\n",
    "ORDER BY salary DESC\n",
    "LIMIT 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Analytics with SQL Magic\n",
    "\n",
    "Now let's run some more complex analytical queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.63 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c78e65d7814338aab1d1c3ac436787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SparkSchemaWidget(nodes=(Node(close_icon='angle-down', close_icon_style='danger', icon='project-diagram', icon‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a class=\"external\" href=\"http://4dfa796b94ef:4040\" target=\"_blank\" >Open Spark UI ‚≠ê LASAGNA-Architecture-Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><table border='1' class='mathjax_ignore'>\n",
       "<tr><th>department</th><th>employee_count</th><th>avg_salary</th><th>avg_bonus</th><th>excellent_performers</th><th>excellent_percentage</th></tr>\n",
       "<tr><td>HR</td><td>1613</td><td>121460.77</td><td>15716.82</td><td>524</td><td>32.49</td></tr>\n",
       "<tr><td>Sales</td><td>1689</td><td>120708.67</td><td>15587.78</td><td>523</td><td>30.97</td></tr>\n",
       "<tr><td>Marketing</td><td>1631</td><td>120615.47</td><td>15543.27</td><td>511</td><td>31.33</td></tr>\n",
       "<tr><td>Finance</td><td>1677</td><td>119786.38</td><td>15552.11</td><td>527</td><td>31.43</td></tr>\n",
       "<tr><td>Engineering</td><td>1674</td><td>119244.7</td><td>15318.62</td><td>517</td><td>30.88</td></tr>\n",
       "<tr><td>Operations</td><td>1716</td><td>117332.43</td><td>15281.02</td><td>496</td><td>28.90</td></tr>\n",
       "</table>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jlse-output\" data-result-id=\"0970b4b60ec9c8384898397f29adda6c1110d7f334b814bf3cbd32b1ecf9ee4f\" style=\"display:none;\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Department analysis with evolved schema columns\n",
    "SELECT \n",
    "    department,\n",
    "    COUNT(*) as employee_count,\n",
    "    ROUND(AVG(salary), 2) as avg_salary,\n",
    "    ROUND(AVG(bonus), 2) as avg_bonus,\n",
    "    COUNT(CASE WHEN performance_rating = 'Excellent' THEN 1 END) as excellent_performers,\n",
    "    ROUND(COUNT(CASE WHEN performance_rating = 'Excellent' THEN 1 END) * 100.0 / COUNT(*), 2) as excellent_percentage\n",
    "FROM iceberg.lasagna_demo.employees_iceberg\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.74 seconds\n",
      "Only showing top 20 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdec610749e445aaf38259eaefa3aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SparkSchemaWidget(nodes=(Node(close_icon='angle-down', close_icon_style='danger', icon='project-diagram', icon‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a class=\"external\" href=\"http://4dfa796b94ef:4040\" target=\"_blank\" >Open Spark UI ‚≠ê LASAGNA-Architecture-Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><table border='1' class='mathjax_ignore'>\n",
       "<tr><th>location</th><th>department</th><th>employee_count</th><th>avg_salary</th><th>max_salary</th><th>min_salary</th></tr>\n",
       "<tr><td>Berlin</td><td>Finance</td><td>163</td><td>122902.68</td><td>199390</td><td>40412</td></tr>\n",
       "<tr><td>Berlin</td><td>HR</td><td>158</td><td>122274.94</td><td>199727</td><td>40942</td></tr>\n",
       "<tr><td>Berlin</td><td>Marketing</td><td>165</td><td>120545.59</td><td>199737</td><td>40522</td></tr>\n",
       "<tr><td>Berlin</td><td>Sales</td><td>174</td><td>120180.92</td><td>194941</td><td>40048</td></tr>\n",
       "<tr><td>Berlin</td><td>Engineering</td><td>157</td><td>120057.38</td><td>197757</td><td>41051</td></tr>\n",
       "<tr><td>Berlin</td><td>Operations</td><td>163</td><td>118091.89</td><td>199971</td><td>40189</td></tr>\n",
       "<tr><td>London</td><td>HR</td><td>163</td><td>124216.86</td><td>199783</td><td>41499</td></tr>\n",
       "<tr><td>London</td><td>Finance</td><td>171</td><td>117967.18</td><td>196911</td><td>42489</td></tr>\n",
       "<tr><td>London</td><td>Engineering</td><td>166</td><td>117438.44</td><td>199189</td><td>40268</td></tr>\n",
       "<tr><td>London</td><td>Sales</td><td>141</td><td>115887.23</td><td>198903</td><td>40669</td></tr>\n",
       "<tr><td>London</td><td>Marketing</td><td>147</td><td>115859.83</td><td>199147</td><td>40092</td></tr>\n",
       "<tr><td>London</td><td>Operations</td><td>187</td><td>112854.63</td><td>199157</td><td>40402</td></tr>\n",
       "<tr><td>New York</td><td>Sales</td><td>185</td><td>127268.14</td><td>198816</td><td>41146</td></tr>\n",
       "<tr><td>New York</td><td>Engineering</td><td>163</td><td>124144.55</td><td>198954</td><td>40801</td></tr>\n",
       "<tr><td>New York</td><td>Marketing</td><td>140</td><td>121077.81</td><td>198505</td><td>41547</td></tr>\n",
       "<tr><td>New York</td><td>Finance</td><td>181</td><td>120739.03</td><td>199811</td><td>41818</td></tr>\n",
       "<tr><td>New York</td><td>Operations</td><td>187</td><td>118659.67</td><td>199960</td><td>40324</td></tr>\n",
       "<tr><td>New York</td><td>HR</td><td>139</td><td>115193.55</td><td>197309</td><td>40392</td></tr>\n",
       "<tr><td>San Francisco</td><td>Sales</td><td>179</td><td>123343.26</td><td>197849</td><td>40256</td></tr>\n",
       "<tr><td>San Francisco</td><td>Marketing</td><td>175</td><td>123066.61</td><td>199995</td><td>40170</td></tr>\n",
       "</table>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jlse-output\" data-result-id=\"f4001c50731c79a0848be22ff966ca9127fcb6c8479ec66ce43b8fb1e33a7054\" style=\"display:none;\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Location-based analysis with partitioning benefits\n",
    "SELECT \n",
    "    location,\n",
    "    department,\n",
    "    COUNT(*) as employee_count,\n",
    "    ROUND(AVG(salary), 2) as avg_salary,\n",
    "    ROUND(MAX(salary), 2) as max_salary,\n",
    "    ROUND(MIN(salary), 2) as min_salary\n",
    "FROM iceberg.lasagna_demo.employees_iceberg\n",
    "WHERE is_active = true\n",
    "GROUP BY location, department\n",
    "HAVING COUNT(*) > 100\n",
    "ORDER BY location, avg_salary DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Table Analysis with SQL Magic\n",
    "\n",
    "Let's demonstrate joining data across different table formats:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Magic Summary\n",
    "\n",
    "The `%%sparksql` magic command provides a Databricks-like experience in JupyterLab, allowing you to:\n",
    "\n",
    "- **Write SQL directly in notebook cells** with automatic result display\n",
    "- **Perform complex analytics** with interactive table results\n",
    "\n",
    "This demonstrates the power of the LASAGNA architecture with modern table formats and interactive SQL capabilities!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
